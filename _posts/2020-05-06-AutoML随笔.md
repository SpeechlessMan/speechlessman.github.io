---
layout:     post
title:      AutoML
subtitle:   第一篇博客
date:       2020-05-06
author:     speechless
header-img: img/home-bg-o.jpg
catalog: true
tags:
    - 随便写写
---

# AutoML

机器学习（machine learning）在过去的十几年中得到了迅速发展，并受到了各行各业的关注。但时至今日，机器学习依旧有着较高的技术门槛，特征工程，模型选择，超参数优化等都需要大量的专业知识作为支撑，因此为了减少人力以及时间投入，自动机器学习（automated machine learning，AutoML）出现了。

AutoML绝对不是今年才出现在，早在2003年，就有研究者提出使用遗传算法进行特征生成与选择[2]。AutoML的目的可以简单理解为：在1）计算资源有限；2）没有人为干预的情况下，使得机器学习模型的性能尽量好[1]。目前对AutoML的研究大致可以分为特征工程（feature engineering），模型选择（model selection）以及超参数优化（hyperparameter optimization）这三个部分。

## Feature Engineering

自动特征工程部分粗略地划分为特征升维与特征降维，可以理解为是一个**expansion-reduction**的过程，**expansion**指扩展特征集，也就是生成特征，最简单的是采用random的方式，从一些预定义的算数运算符，如{log, square, ..., absolute}中随机选取一个运算符${f}$，并随机选取一些特征构成特征子集${F_i}$，将${f(F_i)}$作为新的特征。至于**reduction**，主要是指缩减特征以达到降维的目的，以传统的**Filter**方法为例，可以考察每个特征个预测目标的相关性，删除相关性较小的特征。除此以外，PCA，ICA也可达到降维的目的，但这类方法可能将原始的特征映射到另一特征空间中，每个特征失去了原始的意义。
自动特征工程是一个很有意思的话题，在工业界中有这样一句话，特征工程决定了模型性能的上限，由此也可以看出特征工程的重要性。可以将自动工程任务视作一条pipeline构建任务，pipeline的组件包括简单的特征生成工具与特征降维工具，我们要做的就是根据不同的任务构建对应的流水线。但这样一种端到端的做法忽略了一些关键问题：如特征的可解释性，特征之间的关系等。因此，自动特征工程值得进一步深入的研究。

## others

关于模型选择和超参数优化在这里不做过多的解释，模型选择就是根据不同任务选择合适的模型，或是集成模型，而超参数优化则是解决研究人员头痛的**调参**难题。除了上述所提及的一些研究方向，当下关于AutoML的热门研究方向还有神经架构搜索（neural architecture search，NAS），利用某种策略（如强化学习）搜索合适的神经网络架构。

# 一些题外话

AutoML的主要目的还是为了**减少人力和时间**，以特征工程为例，对那些data scientists来说，提取除好的特征往往是耗时的，且十分依赖**domain knowledge**，而自动特征工程的目的就是在没有领域知识指导的基础上，生成较好的特征，是一个完全由数据驱动（data-driven）的问题。另外，AutoML的出现也不意味着研究人员的失业，至少目前是不会的，以NAS为例，搜索一个适合的神经网络结构的代价是巨大的，可能需要几百个gpu，且耗费上几天的时间，所以AutoML只能说是一个具有前景的未来研究方向。


[1] [Taking Human out of Learning Applications: A Survey on Automated Machine Learning.](https://arxiv.org/abs/1810.13306v1)  
[2] Feature construction and selection using genetic programming and a genetic algorithm, 2003